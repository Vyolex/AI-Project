{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the required packages for our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "from src.evaluate_recommender import evaluate_recommender, generate_gt, map_id_to_name, parse_json\n",
    "from os import cpu_count\n",
    "from os.path import exists\n",
    "qual_eval_folder = './evaluation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the basic recommender using different distance metrics, tf-idf methods and disabling/enabling feedback weighting\n",
    "\n",
    "Distance metrics:\n",
    "- Euclidian distance: `sqrt(sum((x - y)^2))`\n",
    "- Cosine distance: $1-\\frac{x \\cdot y}{||x||_2||y||_2}$\n",
    "- Manhattan distance: `sum(|x - y|)`\n",
    "\n",
    "Tf-idf methods:\n",
    "- No tf-idf\n",
    "- default tf-idf: `tf(t, d) * [log [n/df(t)] + 1]`\n",
    "- smoothed tf-idf: `tf(t, d) * [log [(1+n)/(1+df(t))] + 1]`\n",
    "- sublinear tf-idf: `[1 + log(tf)] * [log [n/df(t)] + 1]`\n",
    "- smoothed sublinear tf-idf: `[1 + log(tf)] * [log [(1+n)/(1+df(t))] + 1]`\n",
    "\n",
    "Feedback weighting: will transform the feature vectors of items that were reviewed negatively to negative values (dislikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean None False: {'nDCG@k': 0.06321204335487078, 'recall@k': 0.007783160974699364}\n",
      "euclidean default False: {'nDCG@k': 0.08159832719330469, 'recall@k': 0.011887017432435563}\n",
      "euclidean smooth False: {'nDCG@k': 0.0816714660357247, 'recall@k': 0.011895705165064914}\n",
      "euclidean sublinear False: {'nDCG@k': 0.08159832719330469, 'recall@k': 0.011887017432435563}\n",
      "euclidean smooth_sublinear False: {'nDCG@k': 0.0816714660357247, 'recall@k': 0.011895705165064914}\n",
      "cosine None False: {'nDCG@k': 0.15207915764003457, 'recall@k': 0.018838551065396947}\n",
      "cosine default False: {'nDCG@k': 0.14191684802318041, 'recall@k': 0.01797538261828906}\n",
      "cosine smooth False: {'nDCG@k': 0.14190747151950622, 'recall@k': 0.017978397262666482}\n",
      "cosine sublinear False: {'nDCG@k': 0.14191684802318041, 'recall@k': 0.01797538261828906}\n",
      "cosine smooth_sublinear False: {'nDCG@k': 0.14190747151950622, 'recall@k': 0.017978397262666482}\n",
      "cosine None True: {'nDCG@k': 0.13767302777223162, 'recall@k': 0.017088377986683268}\n",
      "cosine default True: {'nDCG@k': 0.12975630443897884, 'recall@k': 0.0165154910526756}\n",
      "cosine smooth True: {'nDCG@k': 0.1297395106585815, 'recall@k': 0.016513850492044808}\n",
      "cosine sublinear True: {'nDCG@k': 0.12975630443897884, 'recall@k': 0.0165154910526756}\n",
      "cosine smooth_sublinear True: {'nDCG@k': 0.1297395106585815, 'recall@k': 0.016513850492044808}\n",
      "manhattan None False: {'nDCG@k': 0.06326254365730855, 'recall@k': 0.007798871591447793}\n",
      "manhattan default False: {'nDCG@k': 0.09471225652135498, 'recall@k': 0.013003734476565273}\n",
      "manhattan smooth False: {'nDCG@k': 0.09473405337430038, 'recall@k': 0.012999448160768584}\n",
      "manhattan sublinear False: {'nDCG@k': 0.09471225652135498, 'recall@k': 0.013003734476565273}\n",
      "manhattan smooth_sublinear False: {'nDCG@k': 0.09473405337430038, 'recall@k': 0.012999448160768584}\n"
     ]
    }
   ],
   "source": [
    "gt_file = './data/ground_truth.parquet'\n",
    "if not exists(gt_file):\n",
    "    generate_gt(gt_file)\n",
    "metrics = ['euclidean', 'cosine']\n",
    "tfidf = [None, 'default', 'smooth', 'sublinear', 'smooth_sublinear']\n",
    "combinations = list(itertools.product(metrics, tfidf, [False]))\n",
    "combinations.extend(list(itertools.product(['cosine'], tfidf, [True])))\n",
    "    \n",
    "with Pool(min(cpu_count(), len(combinations))) as pool:\n",
    "    results = [pool.apply_async(evaluate_recommender, args=(metric, tfidf, use_feedback, qual_eval_folder)) for metric, tfidf, use_feedback in combinations]\n",
    "    output = [p.get() for p in results]\n",
    "for result in output:\n",
    "    print(result[0], result[1], result[2], '\\b:', result[3])\n",
    "        \n",
    "with Pool(min(cpu_count(), len(tfidf))) as pool:\n",
    "    results = [pool.apply_async(evaluate_recommender, args=('manhattan', tfidf_method, False, qual_eval_folder)) for tfidf_method in tfidf]\n",
    "    output = [p.get() for p in results]\n",
    "for result in output:\n",
    "    print(result[0], result[1], result[2], '\\b:', result[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional)  Create a .zip archive of the created qualitative evaluation files. This is done such that the qualitative evaluation results can be shared through GitHub.\n",
    "            \n",
    "    This step can be skipped if the file `evaluation.zip` is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(f'{qual_eval_folder}/evaluation', 'zip', f'{qual_eval_folder}/source')\n",
    "shutil.rmtree(f'{qual_eval_folder}/source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The qualitative evaluation results in `evaluation.zip` are provided in terms of item ids.\n",
    "In order to be able to interpret the results, the ids are mapped to application names through the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32135it [00:01, 22409.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 32135 rows.\n"
     ]
    }
   ],
   "source": [
    "shutil.unpack_archive(f'{qual_eval_folder}/evaluation.zip', qual_eval_folder)\n",
    "\n",
    "import glob\n",
    "games = parse_json(\"./data/steam_games.json\")\n",
    "games = games[['id', 'app_name']]\n",
    "mapping = dict(zip(games.id, games.app_name))\n",
    "for f in glob.glob(f'{qual_eval_folder}/*.csv'):\n",
    "    map_id_to_name(mapping, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64dc3e748883430fd73e07c7431dac127012ab88bcfa4187114a89d6e3756f23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
