{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the required packages for our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from multiprocessing import Pool\n",
    "from evaluate_recommender import evaluate_recommender, generate_gt\n",
    "from os import cpu_count\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the basic recommender using different distance metrics and tf-idf methods\n",
    "\n",
    "Distance metrics:\n",
    "    - Euclidian distance\n",
    "    - Cosine distance\n",
    "\n",
    "Tf-idf methods:\n",
    "    - No tf-idf\n",
    "    - default tf-idf\n",
    "    - smoothed tf=idf\n",
    "    - sublinear tf-idf\n",
    "    - smoothed sublinear tf-idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_file = './data/ground_truth.parquet'\n",
    "if not exists(gt_file):\n",
    "    generate_gt(gt_file)\n",
    "metrics = ['euclidean', 'cosine']\n",
    "tfidf = [None, 'default', 'smooth', 'sublinear', 'smooth_sublinear']\n",
    "combinations = list(itertools.product(metrics, tfidf))\n",
    "with Pool(min(cpu_count(), len(combinations))) as pool:\n",
    "    results = [pool.apply_async(evaluate_recommender, args=(metric, tfidf)) for metric, tfidf in combinations]\n",
    "    output = [p.get() for p in results]\n",
    "for result in output:\n",
    "    print(result[0], result[1] + ':', result[2])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
